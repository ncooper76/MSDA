---
title: "DATA 605 HW4"
author: "Nathan Cooper"
date: "September 19, 2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pracma)
```

### Problem Set 1

In this problem, we'll verify using R that SVD and Eigenvalues are related as worked out in the weekly module. Given a $3 x 2$ matrix $\textbf{A}$


$$
A = \begin{bmatrix}
1 && 2 && 3 \\
-1 && 0 && 4
\end{bmatrix}
$$

write code in R to compute $X = AA^{T}$ and $Y = A^{T}A$. Then, compute the eigenvalues and eigenvectors of X and Y using the built-in commands in R. Then, compute the left-singular, singular values, and right-singular vectors of A using the svd command. Examine the two sets of singular vectors and show that they are indeed eigenvectors of X and Y. In addition, the two non-zero eigenvalues (the 3rd value will be very close to zero, if not zero) of both X and Y are the same and are squares of the non-zero singular values of A. Your code should compute all these vectors and scalars and store them in variables. Please add enough comments in your code to show me how to interpret your steps.
 
 > First we will use R to get the transpose. This is done fairly simply by hand, but with more complicated matrices it's better to let the computer do the work.
 
```{r A-T, eval=TRUE}
A <- matrix(c(1,2,3,-1,0,4), nrow = 2, ncol = 3, byrow = TRUE)
A
A_T <- t(A)
A_T
```

> Next we compute X and Y.

```{r X-Y, eval=TRUE}
X = A%*%A_T
X
Y = A_T%*%A
Y
```

> Now we get the Eigenvalues and Vectors of X.

```{r eigenX, eval=TRUE}
eigenX <-eigen(X)
eigenX
eVecX <- eigenX$vectors
eValX <- eigenX$values
```


Sometimes R does the eigenvector values a little more complicated that needed, however  $0.7533635/0.6576043 = 1.14$ so we can't make these eigenvectors look much better.

> The Eigenvectors and -values for Y are:


```{r eigenY, eval=TRUE}
eigenY <- eigen(Y)
eigenY
eVecY <- eigenY$vectors
eValY <- eigenY$values
```

Eigenvalue 3 is probably 0 with some rounding error, in that case the eigenvector can be written 

$$
\begin{bmatrix}
4 \\ -3.5 \\ 1
\end{bmatrix}
$$

Arithmetic on the other two eigenvectors did not result in rational numbers so I will leave them as is.
Note that translating from scientific notation the other eigenvalues are 26.60180 and 4.398198 which are the same as X, as mentioned in the problem text.

```{r compare1, eval=TRUE}
eValX
eValY
```

> N.B., that X and Y are both symmetric matrices, so their Eigenvectors ought to be orthoganal. We can test this by using the dot product.

```{r eigen-dots, eval=TRUE}
eVecX[1,]%*%eVecX[1,]
eVecX[1,]%*%eVecX[2,]
eVecY[1,]%*%eVecY[3,]
eVecX[2,]%*%eVecX[2,]
eVecY[2,]%*%eVecY[3,]
eVecY[3,]%*%eVecY[3,]
```

Within rounding errors of R, $e^{-17}$ is very small, this these conditions are true.

> We will now compute the Left-Singular Values of A. For side-by-side comparisons, I'll include the Eigenvectors of X. The left hand decompostion in svd() is set by letting nu = nrow(A) = 2 and nv = 0.

```{r l-svdA, eval=TRUE}
left_sing <- svd(A, nu = 2, nv = 0)
left_sing
eVecX
```

One differs by a negative, but they are otherwise the same vectors.

> We will now compute the Right-Singular Values of A. For side-by-side comparisons, I'll include the Eigenvectors of Y. The right hand decomposition is set by letting nu = 0 and nv = ncol(A) = 3.

```{r r-svdA, eval=TRUE}
right_sing <- svd(A, nu = 0, nv = 3)
right_sing
eVecY
```

Again, two are different by a factor of -1, but they are otherwise the same.

> This also means that the singular value vectors form an orthonormal set like the eigenvectors.

As for the eigenvalues:

```{r sign-egnVal, eval =TRUE}
(right_sing$d)^2
eValX
(left_sing$d)^2
eValY
```

> As stated in the problem text the non-zero eigenvalues of X and Y are the same as the non-zero singular values.

### Problem Set 2

Using the procedure outlined in section 1 of the weekly handout, write a function to compute the inverse of a well-conditioned full-rank square matrix using co-factors. In order to compute the co-factors, you may use built-in commands to compute the determinant. Your function should have the following signature:

B = myinverse(A)

where A is a matrix and B is its inverse and $AxB = I$. The off-diagonal elements of I should be close to zero, if not zero. Likewise, the diagonal elements should be close to 1, if not 1. Small numerical precision errors are acceptable but the function myinverse should be correct and must use co-factors and determinant of A to compute the inverse. Please submit PS1 and PS2 in an R-markdown document with your first initial and last name.

```{r B-inv, eval=TRUE}
#Make a test matrix first, different everytime:
A <- matrix(sample(0:9, 25,  replace = TRUE), nrow=5, ncol=5)
myinverse <- function(M){
if(is.matrix(M) & nrow(M) == ncol(M) & det(M) != 0){
  print("Calculating Inverse of inputted Matrix:")
  detM <- det(M)
  C <- matrix(0, nrow = nrow(M), ncol = ncol(M))
  for(i in 1:nrow(M)){
    for(j in 1:ncol(M)){
      C[j,i] = (-1)^(i+j)*det(M[-i,-j]) #Calculate the cofactors, assign it to the co-Factor Matrix.
      #Flipping the idecies here saves calculating the transpose later.
    }#for j
  }#for i
  invM = C/detM #This will give the Inverse per the procedure in the reading.
}#if
else{
  print("Invalid Input, Input should be a square Matrix with non-zero determanent")
}#else
  return(invM)
}#myinverse
A
B = myinverse(A)
B
I = A%*%B
I
```

As you can see, The off-diagonals are close to zero as $10^{-13 to-16}$ are very small and due to rounding errors in r, and the main diagonal terms are 1 or very close to 1. This is fairly robust, as I have added an if-else statement to make sure the correct data type is entered and that it meets the conditions to be invertible, square and a non-zero determinant. 