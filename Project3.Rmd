---
title: "Data_Scientist_Salaries"
author: "Silverio Vasquez & Nathan Cooper"
date: "October 17, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(RCurl)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(XML)
library(rvest)
library(stringr)
library(data.table)
```



## Links to Indeed.com to do a salary search for Data Scientist jobs by metro

```{r links}
url_nyc <- "https://www.indeed.com/salaries/Data-Scientist-Salaries,-New-York-NY"
url_sf <- "https://www.indeed.com/salaries/Data-Scientist-Salaries,-San-Francisco-CA"
url_bost <- "https://www.indeed.com/salaries/Data-Scientist-Salaries,-Boston-MA"
url_chi <- "https://www.indeed.com/salaries/Data-Scientist-Salaries,-Chicago-IL"
```

## Scrape webpage results for salary data in NYC, San Fran, Boston, and Chicago:


```{r scrape}
# lists to hold salary results and compensation results from webpages
sal_nyc <- list()
comp_nyc <- list()

sal_sf <- list()
comp_sf <- list()

sal_bost <- list()
comp_bost <- list()

sal_chi <- list()
comp_chi <- list()

# for loop to go through all the search result pages on Indeed.com
# each search result page holds about 10 listings of average salaries
# which explains why the counter 'i' that gets attached to the url
# jumps by 10 each time

# First loop for NYC
# j will act as an index for the list
j=1
for (i in seq(0,70,10)) {
        # first page of results doesn't have a counter in the url
        if (i == 0) link <- url_nyc else link <- paste0(url_nyc,"?start=",i)
        
        # the HTML/CSS is messy, but I found the CSS tags associated with 
        # salary and average compensation
        pg_sal <- read_html(link) %>% html_nodes('.cmp-sal-summary') %>% html_text()
        pg_comp <- read_html(link) %>% html_nodes('.cmp-sal-links') %>% html_text()
        
        # save each table of search results as a list within a list
        sal_nyc[j] <- list(pg_sal)
        comp_nyc[j] <- list(pg_comp)
        
        # increase our index by 1 each time
        j = j + 1
}

# Second loop for San Fran
# j will act as an index for the list
j=1
for (i in seq(0,50,10)) {
        # first page of results doesn't have a counter in the url
        if (i == 0) link <- url_sf else link <- paste0(url_sf,"?start=",i)
        
        # the HTML/CSS is messy, but I found the CSS tags associated with 
        # salary and average compensation
        pg_sal <- read_html(link) %>% html_nodes('.cmp-sal-summary') %>% html_text()
        pg_comp <- read_html(link) %>% html_nodes('.cmp-sal-links') %>% html_text()
        
        # save each table of search results as a list within a list
        sal_sf[j] <- list(pg_sal)
        comp_sf[j] <- list(pg_comp)
        
        # increase our index by 1 each time
        j = j + 1
}

# Third loop for Boston
# j will act as an index for the list
j=1
for (i in seq(0,50,10)) {
        # first page of results doesn't have a counter in the url
        if (i == 0) link <- url_bost else link <- paste0(url_bost,"?start=",i)
        
        # the HTML/CSS is messy, but I found the CSS tags associated with 
        # salary and average compensation
        pg_sal <- read_html(link) %>% html_nodes('.cmp-sal-summary') %>% html_text()
        pg_comp <- read_html(link) %>% html_nodes('.cmp-sal-links') %>% html_text()
        
        # save each table of search results as a list within a list
        sal_bost[j] <- list(pg_sal)
        comp_bost[j] <- list(pg_comp)
        
        # increase our index by 1 each time
        j = j + 1
}

# Fourth loop for Chicago
# j will act as an index for the list
j=1
for (i in seq(0,20,10)) {
        # first page of results doesn't have a counter in the url
        if (i == 0) link <- url_chi else link <- paste0(url_chi,"?start=",i)
        
        # the HTML/CSS is messy, but I found the CSS tags associated with 
        # salary and average compensation
        pg_sal <- read_html(link) %>% html_nodes('.cmp-sal-summary') %>% html_text()
        pg_comp <- read_html(link) %>% html_nodes('.cmp-sal-links') %>% html_text()
        
        # save each table of search results as a list within a list
        sal_chi[j] <- list(pg_sal)
        comp_chi[j] <- list(pg_comp)
        
        # increase our index by 1 each time
        j = j + 1
}

```

## Now that the data has been scraped from the web, it's time to clean it up

```{r cleanup}
####################################################
# Cleanup NYC Data
####################################################

# following two lines turn the nest listed into a dataframe
dfs <- lapply(comp_nyc, data.frame, stringsAsFactors = FALSE)
y <- bind_rows(dfs)

# ugly solution to clean up extra text after name of company...
colnames(y) <- "company"
a <- str_split_fixed(y$company,'-',2)
colnames(a) <- c('name','misc')
a <- a[,-2]
a <- str_split_fixed(a,' Jobs',2)

# following two lines turn the nest listed into a dataframe
dfs2 <- lapply(sal_nyc, data.frame, stringsAsFactors = FALSE)
z <- bind_rows(dfs2)
colnames(z) <- "salary"

# removes the row of national wide average that appears on every search result page
z2 <- data.frame(z[!grepl("Average", z$salary),])
colnames(z2) <- "salary"

# combines company and offered Data Scientist salary in a 2-column dataframe
nyc <- data.frame(cbind(a[,1], z2))
colnames(nyc) <- c('company','salary')

nyc$salary <- gsub('^[0-9]+','',as.character(nyc$salary))
nyc[] <- lapply(nyc, as.character)
nyc$salary <- as.numeric(unlist(str_replace_all(str_extract_all(nyc$salary, '[[0-9]+,.]{2,}'),',','')))

# write.csv(nyc,file="nyc_company_salary.csv")

####################################################
# Now same cleanup for San Fran
####################################################

# following two lines turn the nest listed into a dataframe
dfs <- lapply(comp_sf, data.frame, stringsAsFactors = FALSE)
y <- bind_rows(dfs)

# ugly solution to clean up extra text after name of company...
colnames(y) <- "company"
a <- str_split_fixed(y$company,'-',2)
colnames(a) <- c('name','misc')
a <- a[,-2]
a <- str_split_fixed(a,' Jobs',2)

# following two lines turn the nest listed into a dataframe
dfs2 <- lapply(sal_sf, data.frame, stringsAsFactors = FALSE)
z <- bind_rows(dfs2)
colnames(z) <- "salary"

# removes the row of national wide average that appears on every search result page
z2 <- data.frame(z[!grepl("Average", z$salary),])
colnames(z2) <- "salary"

# combines company and offered Data Scientist salary in a 2-column dataframe
sf <- data.frame(cbind(a[,1], z2))
colnames(sf) <- c('company','salary')

sf$salary <- gsub('^[0-9]+','',as.character(sf$salary))
sf[] <- lapply(sf, as.character)

sf$salary <- as.numeric(unlist(str_replace_all(str_extract_all(sf$salary, '[[0-9]+,.]{2,}'),',','')))

# Annualize per hour compensation rate by assuming a 40-hour work week
sf$salary[48] <- sf$salary[48]*40*52
sf$salary[50] <- sf$salary[50]*40*52

# write.csv(sf,file="sf_company_salary.csv")

####################################################
# Now same cleanup for Boston
####################################################

# following two lines turn the nest listed into a dataframe
dfs <- lapply(comp_bost, data.frame, stringsAsFactors = FALSE)
y <- bind_rows(dfs)

# ugly solution to clean up extra text after name of company...
colnames(y) <- "company"
a <- str_split_fixed(y$company,'-',2)
colnames(a) <- c('name','misc')
a <- a[,-2]
a <- str_split_fixed(a,' Jobs',2)

# following two lines turn the nest listed into a dataframe
dfs2 <- lapply(sal_bost, data.frame, stringsAsFactors = FALSE)
z <- bind_rows(dfs2)
colnames(z) <- "salary"

# removes the row of national wide average that appears on every search result page
z2 <- data.frame(z[!grepl("Average", z$salary),])
colnames(z2) <- "salary"

# combines company and offered Data Scientist salary in a 2-column dataframe
bost <- data.frame(cbind(a[,1], z2))
colnames(bost) <- c('company','salary')

bost$salary <- gsub('^[0-9]+','',as.character(bost$salary))
bost[] <- lapply(bost, as.character)

bost$salary <- as.numeric(unlist(str_replace_all(str_extract_all(bost$salary, '[[0-9]+,.]{2,}'),',','')))

# write.csv(bost,file="bost_company_salary.csv")

####################################################
# Now same cleanup for Chicago
####################################################

# following two lines turn the nest listed into a dataframe
dfs <- lapply(comp_chi, data.frame, stringsAsFactors = FALSE)
y <- bind_rows(dfs)

# ugly solution to clean up extra text after name of company...
colnames(y) <- "company"
a <- str_split_fixed(y$company,'-',2)
colnames(a) <- c('name','misc')
a <- a[,-2]
a <- str_split_fixed(a,' Jobs',2)

# following two lines turn the nest listed into a dataframe
dfs2 <- lapply(sal_chi, data.frame, stringsAsFactors = FALSE)
z <- bind_rows(dfs2)
colnames(z) <- "salary"

# removes the row of national wide average that appears on every search result page
z2 <- data.frame(z[!grepl("Average", z$salary),])
colnames(z2) <- "salary"

# combines company and offered Data Scientist salary in a 2-column dataframe
chi <- data.frame(cbind(a[,1], z2))
colnames(chi) <- c('company','salary')

chi[] <- lapply(chi, as.character)
chi$salary <- as.numeric(unlist(str_replace_all(str_extract_all(chi$salary, '[[0-9]+,.]{2,}'),',','')))

# Annualize per hour compensation rate by assuming a 40-hour work week
chi$salary[18] <- chi$salary[18]*40*52

# write.csv(chi,file="chi_company_salary.csv")
```

## The histogram charts below show wide dispersions Data Scientist salaries in these four metros

```{r histograms}
options(scipen = 9)

qplot(nyc$salary, geom="histogram",  xlab="Salaries",
      main = "Histogram for NYC Data Scientist Salaries", fill=I("blue"), 
      col=I("black"), xlim=c(min(nyc$salary)-10000,max(nyc$salary)+10000),
      breaks=seq(min(nyc$salary)-10000, max(nyc$salary)+10000,by=5000))

qplot(sf$salary, geom="histogram", xlab="Salaries",
      main = "Histogram for San Fran Data Scientist Salaries", fill=I("blue"), 
      col=I("black"), xlim=c(min(sf$salary)-10000,max(sf$salary)+10000),
      breaks=seq(min(sf$salary)-10000, max(sf$salary)+10000,by=10000))

qplot(bost$salary, geom="histogram", xlab="Salaries",
      main = "Histogram for Boston Data Scientist Salaries", fill=I("blue"), 
      col=I("black"), xlim=c(min(bost$salary)-10000,max(bost$salary)+10000),
      breaks=seq(min(bost$salary)-10000, max(bost$salary)+10000,by=10000))

qplot(chi$salary, geom="histogram", xlab="Salaries",
      main = "Histogram for Chicago Data Scientist Salaries", fill=I("blue"), 
      col=I("black"), xlim=c(min(chi$salary)-10000,max(chi$salary)+10000),
      breaks=seq(min(chi$salary)-10000, max(chi$salary)+10000,by=10000))

```

### Prelimanary Analysis of Salary Data

To make a more fair comparision of salaries we must take into account cost of living for each city. We will first gather the cost of living indexes from https://www.numbeo.com/cost-of-living/region_rankings.jsp?title=2017-mid&region=019. Note that webscrapping the entire table is costly in terms of time and computer resources, so we will hand pick the cities we need: New York City, San Fransisco, Boston, and Chicago. Also the number are reported as percentages, so we will take the additional step of dividing by 100 to put them into decimal format.

```{r col_index, eval=TRUE}
col_index <- data.frame(matrix(c(c('NYC', 'SanFran', 'Boston', 'Chicago'), c(100.00/100, 101.94/100, 90.23/100,84.39/100)), ncol = 2))
col_index <- col_index %>% rename('City' = X1,'COL.Index' = X2)
col_index[,2] <- col_index[,2] %>% as.character() %>% as.numeric() 
col_index
```


### Confidence Intervals Unadjusted for Cost of Living

We will begin our comparision by calculating 95% Confidence Intervals without Adjusting for Cost of Living.

#### New York City


```{r nyc_ci_un, eval=TRUE}
nyc_mean <- nyc$salary %>% mean()
nyc_mean
nyc_sd <- nyc$salary %>% sd()
nyc_sd
nyc_lower <- nyc_mean - 1.96*nyc_sd
nyc_upper <- nyc_mean + 1.96*nyc_sd
nyc_ci <- c(nyc_lower, nyc_upper)
nyc_ci
```

#### San Fransisco

```{r sf_ci_un, eval=TRUE}
sf_mean <- sf$salary %>% mean()
sf_mean
sf_sd <- sf$salary %>% sd()
sf_sd
sf_lower <- sf_mean - 1.96*sf_sd
sf_upper <- sf_mean + 1.96*sf_sd
sf_ci <- c(sf_lower, sf_upper)
sf_ci
```


#### Boston


```{r bost_ci_un, eval=TRUE}
bost_mean <- bost$salary %>% mean()
bost_mean
bost_sd <- bost$salary %>% sd()
bost_sd
bost_lower <- bost_mean - 1.96*bost_sd
bost_upper <- bost_mean + 1.96*bost_sd
bost_ci <- c(bost_lower, bost_upper)
bost_ci
```

#### Chicago


```{r chi_ci_un, eval=TRUE}
chi_mean <- chi$salary %>% mean()
chi_mean
chi_sd <- chi$salary %>% sd()
chi_sd
chi_lower <- chi_mean - 1.96*chi_sd
chi_upper <- chi_mean + 1.96*chi_sd
chi_ci <- c(chi_lower, chi_upper)
chi_ci
```

There is quite a bit of overlap between all cities, so we expect that they should be consistant with each other regarding a student's t-test.

### Student t-tests and K-S tests of the Salary Data Unadjusted for Cost of Living Idex

Student's t-tests are used to check if the means of two samples are different from each other. The Null Hyopothesis is that the true diffenence in means of the populations the samples are drawn from is 0. The alternatibe hypothesis is that the difference in the population means is not zero. If the null is rejected you can infere that the two populations are different. For the t-test to be valid the samples must have a Normal Distribution and have similar variences. 

Kologomorov-Smirnov (KS) tests test the same null and alternative hypothesis as the Student's t-test. The KS-test analyzes differences in the Cumulative Distribution Function (CDF) of the two samples. Unlike the t-test, you do not have to make any assumptions about the samples.

We need to analyze pairs out of 4 cities this invovles 6 combinations:

```{r combn, eval=TRUE}
combn(4,2)
```

#### New York and San Fransisco t-test

```{r t_n_s, eval=TRUE}
t.test(nyc$salary,sf$salary)
```
According to Student's t-test New York's and San Fransisco's salaries are similar before accounting for cost of living.

#### New York and Boston t-test

```{r t_n_b, eval=TRUE}
t.test(nyc$salary,bost$salary)
```
According to Student's t-test New York's and Boston's salaries are different before accounting for cost of living.

#### New York and Chicago t-test

```{r t_n_c, eval=TRUE}
t.test(nyc$salary,chi$salary)
```
According to Student's t-test New York's and Chicago's salaries are different before accounting for cost of living.

#### San Fransisco and Boston t-test

```{r t_s_b, eval=TRUE}
t.test(sf$salary,bost$salary)
```
According to Student's t-test San Fransisco's and Boston's salaries are different before accounting for cost of living.

#### San Fransisco and Chicago t-test

```{r t_s_c, eval=TRUE}
t.test(sf$salary,chi$salary)
```
According to Student's t-test San Fransisco's and Chicago's salaries are different before accounting for cost of living.

#### Boston and Chicago t-test

```{r t_b_c, eval=TRUE}
t.test(bost$salary,chi$salary)
```

Boston and Chicago's Salaries are similar under Student's t-test.

```{r box_pl, eval=TRUE}
# par(mfrow = c(4,1))
# boxplot(nyc$salary, horizontal = TRUE)
# boxplot(sf$salary, horizontal = TRUE)
# boxplot(bost$salary, horizontal = TRUE)
# boxplot(chi$salary, horizontal = TRUE)
```

#### New York and San Fransisco KS-test

```{r k_n_s, eval=TRUE}
ks.test(nyc$salary,sf$salary, exact = TRUE)
```
According to the KS test New York's and San Fransisco's salaries are similar before accounting for cost of living.

#### New York and Boston KS-test

```{r k_n_b, eval=TRUE}
ks.test(nyc$salary,bost$salary, exact = TRUE)
```
According to the KS test New York's and Boston's salaries are different before accounting for cost of living.

#### New York and Chicago KS-test

```{r k_n_c, eval=TRUE}
ks.test(nyc$salary,chi$salary, exact = TRUE)
```
According to the KS test New York's and Chicago's salaries are different before accounting for cost of living.

#### San Fransisco and Boston KS-test

```{r k_s_b, eval=TRUE}
ks.test(sf$salary,bost$salary, exact = TRUE)
```
According to the KS test San Fransisco's and Boston's salaries are different before accounting for cost of living.

#### San Fransisco and Chicago KS-test

```{r k_s_c, eval=TRUE}
ks.test(sf$salary,chi$salary, exact = TRUE)
```
According to the KS test San Fransisco's and Chicago's salaries are different before accounting for cost of living.

#### Boston and Chicago KS-test

```{r k_b_c, eval=TRUE}
ks.test(bost$salary,chi$salary, exact = TRUE)
```

According to the KS test Boston's and Chicago's salaries are different before accounting for cost of living.

Regardless of the test used, only New York and San Fransisco had similar salary distributions.

### Adjusting for Cost of Living

Now we will take the salary data and divide by the Cost of Livining index. In this way lower cost of living salaries (i.e., < 1) will be increased to reflect more purchasing power per dollar, and cities with higher cost of living (>1) will have there salaries decreased to reflect less purchasing power per dollar.

```{r col_adj, eval=TRUE}
head(nyc$salary)
adj_nyc <- nyc$salary/col_index[1,2]
head(adj_nyc)
head(sf$salary)
adj_sf <- sf$salary/col_index[2,2]
head(adj_sf)
head(bost$salary)
adj_bost <- bost$salary/col_index[3,2]
head(adj_bost)
head(chi$salary)
adj_chi <- chi$salary/col_index[4,2]
head(adj_chi)
```
### Confidence Intervals Adjusted for Cost of Living

We will begin our comparision by calculating 95% Confidence Intervals without Adjusting for Cost of Living.

#### New York City


```{r nyc_ci_adj, eval=TRUE}
nyc_mean_adj <- adj_nyc %>% mean()
nyc_mean_adj
nyc_sd_adj <- adj_nyc %>% sd()
nyc_sd_adj
nyc_lower_adj <- nyc_mean_adj - 1.96*nyc_sd_adj
nyc_upper_adj <- nyc_mean_adj + 1.96*nyc_sd_adj
nyc_ci_adj <- c(nyc_lower_adj, nyc_upper_adj)
nyc_ci_adj
```

#### San Fransisco

```{r sf_ci_adj, eval=TRUE}
sf_mean_adj <- adj_sf %>% mean()
sf_mean_adj
sf_sd_adj <- adj_sf %>% sd()
sf_sd_adj
sf_lower_adj <- sf_mean_adj - 1.96*sf_sd_adj
sf_upper_adj <- sf_mean_adj + 1.96*sf_sd_adj
sf_ci_adj <- c(sf_lower_adj, sf_upper_adj)
sf_ci_adj
```


#### Boston


```{r bost_ci_adj, eval=TRUE}
bost_mean_adj <- adj_bost %>% mean()
bost_mean_adj
bost_sd_adj <- adj_bost %>% sd()
bost_sd_adj
bost_lower_adj <- bost_mean_adj - 1.96*bost_sd_adj
bost_upper_adj <- bost_mean_adj + 1.96*bost_sd_adj
bost_ci_adj <- c(bost_lower_adj, bost_upper_adj)
bost_ci_adj
```

#### Chicago


```{r chi_ci_adj, eval=TRUE}
chi_mean_adj <- adj_chi %>% mean()
chi_mean_adj
chi_sd_adj <- adj_chi %>% sd()
chi_sd_adj
chi_lower_adj <- chi_mean_adj - 1.96*chi_sd_adj
chi_upper_adj <- chi_mean_adj + 1.96*chi_sd_adj
chi_ci_adj <- c(chi_lower_adj, chi_upper_adj)
chi_ci_adj
```

There is quite a bit of overlap between all cities, however we did see some statiscally significant differences even when the CI's overlapped in the pervious section.

### Student t-tests and K-S tests of the Salary Data Adjusted for Cost of Living Idex

Here we repeat the analysis from above with salary data that has been adjusted for cost of living. 


#### New York and San Fransisco t-test

```{r ta_n_s, eval=TRUE}
t.test(adj_nyc,adj_sf)
```
According to Student's t-test New York's and San Fransisco's salaries are similar after accounting for cost of living.

#### New York and Boston t-test

```{r ta_n_b, eval=TRUE}
t.test(adj_nyc,adj_bost)
```
According to Student's t-test New York's and Boston's salaries are different after accounting for cost of living. Hoever, the margin has dropped quite a bit.

#### New York and Chicago t-test

```{r ta_n_c, eval=TRUE}
t.test(adj_nyc,adj_chi)
```
According to Student's t-test New York's and Chicago's salaries are not different after accounting for cost of living.

#### San Fransisco and Boston t-test

```{r ta_s_b, eval=TRUE}
t.test(adj_sf,adj_bost)
```
According to Student's t-test San Fransisco's and Boston's salaries are not different after accounting for cost of living.

#### San Fransisco and Chicago t-test

```{r ta_s_c, eval=TRUE}
t.test(adj_sf,adj_chi)
```
According to Student's t-test San Fransisco's and Chicago's salaries are not different after accounting for cost of living.

#### Boston and Chicago t-test

```{r ta_b_c, eval=TRUE}
t.test(adj_bost,adj_chi)
```

Boston and Chicago's Salaries are no longer similar under Student's t-test when adjusting for cost of living.

#### New York and San Fransisco KS-test

```{r ka_n_s, eval=TRUE}
ks.test(adj_nyc,adj_sf, exact = TRUE)
```
According to the KS test New York's and San Fransisco's salaries are similar after accounting for cost of living.

#### New York and Boston KS-test

```{r ka_n_b, eval=TRUE}
ks.test(adj_nyc,adj_bost, exact = TRUE)
```
According to the KS test New York's and Boston's salaries are different after accounting for cost of living.

#### New York and Chicago KS-test

```{r ka_n_c, eval=TRUE}
ks.test(adj_nyc,adj_chi, exact = TRUE)
```
According to the KS test New York's and Chicago's salaries are not different after accounting for cost of living.

#### San Fransisco and Boston KS-test

```{r ka_s_b, eval=TRUE}
ks.test(adj_sf,adj_bost, exact = TRUE)
```
According to the KS test San Fransisco's and Boston's salaries are different after accounting for cost of living.

#### San Fransisco and Chicago KS-test

```{r ka_s_c, eval=TRUE}
ks.test(adj_sf,adj_chi, exact = TRUE)
```
According to the KS test San Fransisco's and Chicago's salaries are not different after accounting for cost of living.

#### Boston and Chicago KS-test

```{r ka_b_c, eval=TRUE}
ks.test(adj_bost,adj_chi, exact = TRUE)
```

According to the KS test Boston's and Chicago's salaries are different after accounting for cost of living.

After adjusting for Cost of Living, Chicago became similar to New York and San Fransisco but Boston remained different.
